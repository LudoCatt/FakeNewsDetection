mse<-function(x){
return(var(x)+((mean(x))-m(x))^2)
}
b.nw = apply(estnw, 1, mse)
b.lp = apply(estlp, 1, mse)
b.ss = apply(estss, 1, mse)
plot(x, b.nw, type = 'l', col = 4, xlab = 'x', ylab = 'mse', main = 'MSE', lty='solid')
b.nw = apply(estnw, 1, mean)-m(x)
b.lp = apply(estlp, 1, mean)-m(x)
b.ss = apply(estss, 1, mean)-m(x)
v.nw = apply(estnw, 1, var)
v.lp = apply(estlp, 1, var)
v.ss = apply(estss, 1, var)
m.nw = b.nw^2+v.nw
m.lp = b.nw^2+v.lp
m.ss = b.nw^2+v.ss
plot(x, v.nw, type = 'l', col = 4, xlab = 'x', ylab = 'mse', main = 'MSE', lty='solid')
lines(x, v.lp, type = 'l', col = 5, lty = 'solid')
lines(x, v.ss, type = 'l', col = 6, lty = 'solid')
legend('topright', legend = c('Nadaraya-Watson', 'Local Polynomial', 'Smoothing Splines'),
col = c(4, 5, 6), lty = c('solid', 'solid', 'solid'))
m.nw = apply(estnw, 1, mean)
m.lp = apply(estlp, 1, mean)
m.ss = apply(estss, 1, mean)
plot(x, m.nw, type = 'l', col = 4, xlab = 'x', ylab = 'm(x)', main = 'Average estimate', lty='solid')
lines(x, m.lp, type = 'l', col = 5, lty = 'solid')
lines(x, m.ss, type = 'l', col = 6, lty = 'solid')
legend('topright', legend = c('Nadaraya-Watson', 'Local Polynomial', 'Smoothing Splines'),
col = c(4, 5, 6), lty = c('solid', 'solid', 'solid'))
estnw <- estlp <- estss <- matrix(0, nrow = 101, ncol = nrep)
senw[,i]<-matrix(0, nrow = 101, ncol = nrep)
estnw <- estlp <- estss <- matrix(0, nrow = 101, ncol = nrep)
senw<-matrix(0, nrow = 101, ncol = nrep)
for(i in 1:nrep) {
## Simulate y-values
y <- m(x) + rnorm(length(x))
## Get estimates for the mean function m(x) in the points given by the vector x
estnw[,i] <- ksmooth(x, y, kernel = "normal", bandwidth = b, x.points = x)$y
estlp[,i] <- predict(loess(y~x, span = span), newdata = x)
estss[,i] <- predict(smooth.spline(x, y, spar = spar), x = x)$y
sigma2nw <- sum((y-estnw[,i])^2) / (length(y) - sum(diag(Snw)))
senw[,i] <- sqrt(sigma2nw * diag(Snw))
}
# S matrix for NW
n <- 101
Snw <- matrix(0, nrow = n, ncol = n)
In <- diag(n) ##  n x n identity matrix
for(j in 1:n) {
y <- In[,j]
Snw[,j] <- ksmooth(x, y, kernel = "normal", bandwidth = 0.2, x.points = x)$y
}
# S matrix for LP
Slp <- matrix(0, nrow = n, ncol = n)
In <- diag(n) ##  n x n identity matrix
for(j in 1:n) {
y <- In[,j]
Slp[,j] <- predict(loess(y~x, span = span), newdata = x)
}
# S matrix for SS
Sss <- matrix(0, nrow = n, ncol = n)
In <- diag(n) ##  n x n identity matrix
for(j in 1:n) {
y <- In[,j]
Sss[,j] <- predict(smooth.spline(x, y, spar = spar), x = x)$y
}
sess<-selp<-senw<-matrix(0, nrow = 101, ncol = nrep)
for(i in 1:nrep) {
## Simulate y-values
y <- m(x) + rnorm(length(x))
## Get estimates for the mean function m(x) in the points given by the vector x
estnw[,i] <- ksmooth(x, y, kernel = "normal", bandwidth = b, x.points = x)$y
estlp[,i] <- predict(loess(y~x, span = span), newdata = x)
estss[,i] <- predict(smooth.spline(x, y, spar = spar), x = x)$y
sigma2nw <- sum((y-estnw[,i])^2) / (length(y) - sum(diag(Snw)))
senw[,i] <- sqrt(sigma2nw * diag(Snw))
sigma2lp <- sum((y-estlp[,i])^2) / (length(y) - sum(diag(Slp)))
selp[,i] <- sqrt(sigma2lp * diag(Slp))
sigma2ss <- sum((y-estss[,i])^2) / (length(y) - sum(diag(Sss)))
sess[,i] <- sqrt(sigma2ss * diag(Sss))
}
which(x==0.5)
# confidence intervals
# mˆ (xi) ± 1.96 · sˆe(mˆ (xi))
# x==0.5 for i=76
true = m(0.5)
cinw = sum((estnw[76, i] - 1.96*senw[76, i] <= true) & (estnw[76, i] + 1.96*senw[76, i] >= true))
cilp = sum((estlp[76, i] - 1.96*selp[76, i] <= true) & (estlp[76, i] + 1.96*selp[76, i] >= true))
cinw = sum((estss[76, i] - 1.96*sess[76, i] <= true) & (estss[76, i] + 1.96*sess[76, i] >= true))
cinw
cinw = sum((estnw[76,] - 1.96*senw[76,] <= true) & (estnw[76,] + 1.96*senw[76,] >= true))
cilp = sum((estlp[76,] - 1.96*selp[76,] <= true) & (estlp[76,] + 1.96*selp[76,] >= true))
cinw = sum((estss[76,] - 1.96*sess[76,] <= true) & (estss[76,] + 1.96*sess[76,] >= true))
cinw
cinw
cilp
cinw
rep(0,5)
true = m(0.5)
cinw = sum((estnw[76,] - 1.96*senw[76,] <= true) & (estnw[76,] + 1.96*senw[76,] >= true))
cilp = sum((estlp[76,] - 1.96*selp[76,] <= true) & (estlp[76,] + 1.96*selp[76,] >= true))
ciss = sum((estss[76,] - 1.96*sess[76,] <= true) & (estss[76,] + 1.96*sess[76,] >= true))
cinw
cilp
ciss
cinw <- cilp <- cinw <- rep(0,nrep)
l<-n<-s<-rep(0,n)
for(j in 1:nrep){
for(i in 1:n){
n[i] = (estnw[i,j] - 1.96*senw[i,j] <= true) & (estnw[i,j] + 1.96*senw[i,j] >= true)
l[i] = (estlp[i,j] - 1.96*selp[i,j] <= true) & (estlp[i,j] + 1.96*selp[i,j] >= true)
s[i] = (estss[i,j] - 1.96*sess[i,j] <= true) & (estss[i,j] + 1.96*sess[i,j] >= true)
}
cinw[j]=all(n)
cilp[j]=all(l)
ciss[j]=all(s)
}
sum(cinw)
sum(cilp)
sum(ciss)
i<-j<-0
n[i] = (estnw[i,j] - 1.96*senw[i,j] <= true) & (estnw[i,j] + 1.96*senw[i,j] >= true)
l[i] = (estlp[i,j] - 1.96*selp[i,j] <= true) & (estlp[i,j] + 1.96*selp[i,j] >= true)
s[i] = (estss[i,j] - 1.96*sess[i,j] <= true) & (estss[i,j] + 1.96*sess[i,j] >= true)
n
l
s
i<-j<-1
n[i] = (estnw[i,j] - 1.96*senw[i,j] <= true) & (estnw[i,j] + 1.96*senw[i,j] >= true)
l[i] = (estlp[i,j] - 1.96*selp[i,j] <= true) & (estlp[i,j] + 1.96*selp[i,j] >= true)
s[i] = (estss[i,j] - 1.96*sess[i,j] <= true) & (estss[i,j] + 1.96*sess[i,j] >= true)
s
s
i<-j<-1
n[i] = (estnw[i,j] - 1.96*senw[i,j] <= true) & (estnw[i,j] + 1.96*senw[i,j] >= true)
l[i] = (estlp[i,j] - 1.96*selp[i,j] <= true) & (estlp[i,j] + 1.96*selp[i,j] >= true)
s[i] = (estss[i,j] - 1.96*sess[i,j] <= true) & (estss[i,j] + 1.96*sess[i,j] >= true)
n
(estnw[i,j] - 1.96*senw[i,j] <= true)
i<-j<-50
n[i] = (estnw[i,j] - 1.96*senw[i,j] <= true) & (estnw[i,j] + 1.96*senw[i,j] >= true)
l[i] = (estlp[i,j] - 1.96*selp[i,j] <= true) & (estlp[i,j] + 1.96*selp[i,j] >= true)
s[i] = (estss[i,j] - 1.96*sess[i,j] <= true) & (estss[i,j] + 1.96*sess[i,j] >= true)
i
n
(estnw[i,j] - 1.96*senw[i,j] <= true)
set.seed(79)
x <- sort(c(0.5, -1 + rbeta(50, 2, 2), rbeta(50, 2, 2)))
span = 0.37614
spar = 0.79424
nrep = 1000
estnw <- estlp <- estss <- matrix(0, nrow = 101, ncol = nrep)
for(i in 1:nrep) {
## Simulate y-values
y <- m(x) + rnorm(length(x))
## Get estimates for the mean function m(x) in the points given by the vector x
estnw[,i] <- ksmooth(x, y, kernel = "normal", bandwidth = b, x.points = x)$y
estlp[,i] <- predict(loess(y~x, span = span), newdata = x)
estss[,i] <- predict(smooth.spline(x, y, spar = spar), x = x)$y
}
# average graphs
m.nw = apply(estnw, 1, mean)
m.lp = apply(estlp, 1, mean)
m.ss = apply(estss, 1, mean)
plot(x, m.nw, type = 'l', col = 4, xlab = 'x', ylab = 'm(x)', main = 'Average estimate', lty='solid')
lines(x, m.lp, type = 'l', col = 5, lty = 'solid')
lines(x, m.ss, type = 'l', col = 6, lty = 'solid')
legend('topright', legend = c('Nadaraya-Watson', 'Local Polynomial', 'Smoothing Splines'),
col = c(4, 5, 6), lty = c('solid', 'solid', 'solid'))
# bias graph
b.nw = apply(estnw, 1, mean)-m(x)
b.lp = apply(estlp, 1, mean)-m(x)
b.ss = apply(estss, 1, mean)-m(x)
plot(x, b.nw, type = 'l', col = 4, xlab = 'x', ylab = 'Bias', main = 'Biases', lty='solid')
lines(x, b.lp, type = 'l', col = 5, lty = 'solid')
lines(x, b.ss, type = 'l', col = 6, lty = 'solid')
legend('topright', legend = c('Nadaraya-Watson', 'Local Polynomial', 'Smoothing Splines'),
col = c(4, 5, 6), lty = c('solid', 'solid', 'solid'))
# increases gratly at the boundary
#Connection between curvature m''(x) and bias?
# Variance graph
v.nw = apply(estnw, 1, var)
v.lp = apply(estlp, 1, var)
v.ss = apply(estss, 1, var)
plot(x, v.nw, type = 'l', col = 4, xlab = 'x', ylab = 'Var', main = 'Variances', lty='solid')
lines(x, v.lp, type = 'l', col = 5, lty = 'solid')
lines(x, v.ss, type = 'l', col = 6, lty = 'solid')
legend('topright', legend = c('Nadaraya-Watson', 'Local Polynomial', 'Smoothing Splines'),
col = c(4, 5, 6), lty = c('solid', 'solid', 'solid'))
# Local polynomial kinda sucks
# MSE graph
m.nw = b.nw^2+v.nw
m.lp = b.nw^2+v.lp
m.ss = b.nw^2+v.ss
plot(x, v.nw, type = 'l', col = 4, xlab = 'x', ylab = 'mse', main = 'MSE', lty='solid')
lines(x, v.lp, type = 'l', col = 5, lty = 'solid')
lines(x, v.ss, type = 'l', col = 6, lty = 'solid')
legend('topright', legend = c('Nadaraya-Watson', 'Local Polynomial', 'Smoothing Splines'),
col = c(4, 5, 6), lty = c('solid', 'solid', 'solid'))
## 2
### (a)
set.seed(3) ## takes ~ 12 sec {2023}
rData <- function(n)
rgamma(n, shape = 1.2) ## define for general use
(true.par <- IQR(rData(100000000))) # 1.249026
## 2
### (a)
set.seed(3) ## takes ~ 12 sec {2023}
rData <- function(n)
rgamma(n, shape = 1.2) ## define for general use
(true.par <- IQR(rData(100000000))) # 1.249026
## 2
### (a)
set.seed(3) ## takes ~ 12 sec {2023}
rData <- function(n)
rgamma(n, shape = 1.2) ## define for general use
(true.par <- IQR(rData(100000000))) # 1.249026
### (b)
set.seed(1)
n=40
sample40 = rData(n)
theta_hat = IQR(sample40) # 1.117164
### (c)
require("boot")
tIQR <- function(x, ind)
IQR(x[ind])
res.boot <- boot(data = sample40, statistic = tIQR, R = 10000)
h=0.05
hist (res.boot$t, breaks = "Scott", prob=TRUE) # breaks="Scott" or "FD" preferred!
lines(density(res.boot$t, bw = h), col=2, lwd=2) # Comp.Stat. Ch.2: use "Sheather-Jones"
## visually add true parameter and our "hat theta":
axis(1, at=true.par, labels=quote(theta), col=4, col.axis=4, cex.axis=3/4, line=-.7)
axis(1, at=theta_hat, labels=quote(hat(theta)), col=2, col.axis=2, cex.axis=3/4, line=-.7)
res.boot$t
res.boot$t[1]
set.seed(3)
(true.par <- mean(rgamma(100000000, shape = 2, rate = 2), trim = 0.1))
rData <- function(n)
rgamma(n, shape = 2) ## define for general use
set.seed(1)
sample40 <- rData(40)
(th.hat <- IQR(sample40))
require("boot")
tIQR <- function(x, ind) IQR(x[ind])
# nominal levels at which we want to evaluate coverage
levels <- c(round(seq(0.7, 0.99, 0.01), 2), round(seq(0.992, 0.996, 0.002), 3))
nl <- length(levels)
# desired coverage
conf <- 0.8
# outer layer
M <- 50
# inner layer and final estimate
B <- 500
tIQR <- function(x, ind)
IQR(x[ind])
# function called by the outer bootstrap
# estimates confidence intervals on second layer of bootstrap
boot.outer.fnn <- function(data, ind, B){
res.boot.inner <- boot(data = data[ind], statistic = tIQR, R = B,
sim = "ordinary")
bci <- boot.ci(res.boot.inner, conf = levels,
type = c("basic"))
ci <- bci[["basic"]][,4:5]
# returns theta_hat*, all lower bound and all upper bounds
out <- c(res.boot.inner$t0, ci[,1], ci[,2])
names(out) <- c("IQR", paste("lower", levels, sep="_"), paste("upper", levels, sep="_"))
out
}
# function to find 1 - alpha'
get.level.prime <- function(res.boot.outer, conf){
thh <- res.boot.outer$t0[1]
lower <- res.boot.outer$t
upper <- res.boot.outer$t
included <- lower <= thh & thh <= upper
colnames(included) <- levels
cover <- apply(included, 1, mean)
if(max(cover) >= conf){
level.prime <- min(levels[cover >= conf])
} else {
# if we are unable to to receive the desired coverage
level.prime <- NA
}
level.prime
}
# get the confidence interval on the nominal levels 1-alpha and 1-alpha' based on B samples
# this can be pulled from $t0
get.ci <- function(res.boot.outer, conf, level.prime) {
if(!is.na(level.prime)) {
level.indices <- c(which(levels == conf), which(levels == level.prime))
} else {
# if we don't find any applicable level, we cheat and use the highest available
level.indices <- c(which(levels == conf), nl)
}
lower0 <- res.boot.outer$t0[2:(nl + 1)]
upper0 <- res.boot.outer$t0[(nl + 2):(2*nl + 1)]
matrix(cbind(lower0[level.indices], upper0[level.indices]), nrow = 2,
dimnames =  list(c("basic", "double"), c("lower", "upper")))
}
res.boot.outer <-  boot(sample40, statistic = tIQR, R = M, sim = "ordinary")
level.prime <- get.level.prime(res.boot.outer, conf)
get.level.prime <- function(res.boot.outer, conf){
thh <- res.boot.outer$t0[1]
lower <- res.boot.outer$t[,1]
upper <- res.boot.outer$t[,1]
included <- lower <= thh & thh <= upper
colnames(included) <- levels
cover <- apply(included, 1, mean)
if(max(cover) >= conf){
level.prime <- min(levels[cover >= conf])
} else {
# if we are unable to to receive the desired coverage
level.prime <- NA
}
level.prime
}
# get the confidence interval on the nominal levels 1-alpha and 1-alpha' based on B samples
# this can be pulled from $t0
get.ci <- function(res.boot.outer, conf, level.prime) {
if(!is.na(level.prime)) {
level.indices <- c(which(levels == conf), which(levels == level.prime))
} else {
# if we don't find any applicable level, we cheat and use the highest available
level.indices <- c(which(levels == conf), nl)
}
lower0 <- res.boot.outer$t0[2:(nl + 1)]
upper0 <- res.boot.outer$t0[(nl + 2):(2*nl + 1)]
matrix(cbind(lower0[level.indices], upper0[level.indices]), nrow = 2,
dimnames =  list(c("basic", "double"), c("lower", "upper")))
}
res.boot.outer <-  boot(sample40, statistic = tIQR, R = M, sim = "ordinary")
level.prime <- get.level.prime(res.boot.outer, conf)
# get the confidence interval on the nominal levels 1-alpha and 1-alpha' based on B samples
# this can be pulled from $t0
get.ci <- function(res.boot.outer, conf, level.prime) {
if(!is.na(level.prime)) {
level.indices <- c(which(levels == conf), which(levels == level.prime))
} else {
# if we don't find any applicable level, we cheat and use the highest available
level.indices <- c(which(levels == conf), nl)
}
lower0 <- res.boot.outer$t0[2:(nl + 1)]
upper0 <- res.boot.outer$t0[(nl + 2):(2*nl + 1)]
matrix(cbind(lower0[level.indices], upper0[level.indices]), nrow = 2,
dimnames =  list(c("basic", "double"), c("lower", "upper")))
}
res.boot.outer <-  boot(sample40, statistic = tIQR, R = M, sim = "ordinary")
level.prime <- get.level.prime(res.boot.outer, conf)
get.ci(res.boot.outer, conf, level.prime)
# function to find 1 - alpha'
get.level.prime <- function(res.boot.outer, conf){
thh <- res.boot.outer$t0[1]
lower <- res.boot.outer$t[,1]
upper <- res.boot.outer$t[,1]
included <- lower <= thh & thh <= upper
colnames(included) <- levels
cover <- apply(included, 2, mean)
if(max(cover) >= conf){
level.prime <- min(levels[cover >= conf])
} else {
# if we are unable to to receive the desired coverage
level.prime <- NA
}
level.prime
}
# get the confidence interval on the nominal levels 1-alpha and 1-alpha' based on B samples
# this can be pulled from $t0
get.ci <- function(res.boot.outer, conf, level.prime) {
if(!is.na(level.prime)) {
level.indices <- c(which(levels == conf), which(levels == level.prime))
} else {
# if we don't find any applicable level, we cheat and use the highest available
level.indices <- c(which(levels == conf), nl)
}
lower0 <- res.boot.outer$t0[2:(nl + 1)]
upper0 <- res.boot.outer$t0[(nl + 2):(2*nl + 1)]
matrix(cbind(lower0[level.indices], upper0[level.indices]), nrow = 2,
dimnames =  list(c("basic", "double"), c("lower", "upper")))
}
res.boot.outer <-  boot(sample40, statistic = tIQR, R = M, sim = "ordinary")
level.prime <- get.level.prime(res.boot.outer, conf)
################################################################################
# Es 3
################################################################################
# Parametric bootstrap
# The values are rounded to minutes (from 2000 to 2018).
boogg <- c(17, 26, 12, 6, 12, 18, 10, 12, 26, 13, 13, 11, 12, 35, 7, 21, 44, 10, 21)
require(MASS)
stripchart(boogg, method = "stack")
fit.gamma <- fitdistr(boogg, "gamma")
### b
hist(boogg, prob = TRUE)
lines(x = seq(0,50,1000), y = dgamma(x))
x=seq(0,50,1000)
lines(x = x, y = dgamma(x))
lines(x = x, y = dgamma(x, shape = fit.gamma))
View(fit.gamma)
View(fit.gamma)
lines(x = x, y = dgamma(x=x, shape = fit.gamma$estimate))
y = dgamma(x=x, shape = fit.gamma$estimate)
x=seq(0,50,1000)
x
x=seq(from=0,to=50,length.out=1000)
lines(x = x, y = dgamma(x=x, shape = fit.gamma$estimate))
hist(boogg, prob = TRUE)
x=seq(from=0,to=50,length.out=100)
lines(x = x, y = dgamma(x=x, scale = fit.gamma$estimate))
hist(boogg, prob = TRUE)
x=seq(from=0,to=50,length.out=100)
lines(x = x, y = dgamma(x=x, shape = fit.gamma$estimate))
abline(v = qgamma(...))
lines(x = x, y = dgamma(x=x, shape = fit.gamma$estimate), type = '--')
hist(boogg, prob = TRUE)
x=seq(from=0,to=50,length.out=100)
lines(x = x, y = dgamma(x=x, shape = fit.gamma$estimate))
abline(v = qgamma(0.75, shape = fit.gamma$estimate)))
hist(boogg, prob = TRUE)
x=seq(from=0,to=50,length.out=100)
lines(x = x, y = dgamma(x=x, shape = fit.gamma$estimate))
abline(v = qgamma(0.75, shape = fit.gamma$estimate))
hist(boogg, prob = TRUE)
x=seq(from=0,to=50,length.out=100)
lines(x = x, y = dgamma(x=x, shape = fit.gamma$estimate))
abline(v = qgamma(0.75, shape = fit.gamma$estimate, col='red'))
### b
hist(boogg, prob = TRUE)
x=seq(from=0,to=50,length.out=100)
lines(x = x, y = dgamma(x=x, shape = fit.gamma$estimate))
abline(v = qgamma(0.75, shape = fit.gamma$estimate),col='red')
hist(boogg, prob = TRUE)
x=seq(from=0,to=50,length.out=100)
lines(x = x, y = dgamma(x=x, shape = fit.gamma$estimate$shape, rate=fit.gamma$estimate$rate))
fit.gamma$estimate
fit.gamma$estimate['shape']
hist(boogg, prob = TRUE)
x=seq(from=0,to=50,length.out=100)
lines(x = x, y = dgamma(x=x, shape = fit.gamma$estimate['shape'], rate=fit.gamma$estimate['rate']))
abline(v = qgamma(0.75, shape = fit.gamma$estimate['shape'], rate=fit.gamma$estimate['rate']), col='red')
## a
boogg <- c(17, 26, 12, 6, 12, 18, 10, 12, 26, 13, 13, 11, 12, 35, 7, 21, 44, 10, 21)
require(MASS)
stripchart(boogg, method = "stack")
fit.gamma <- fitdistr(boogg, "gamma")
### b
hist(boogg, prob = TRUE)
x=seq(from=0,to=50,length.out=100)
lines(x = x, y = dgamma(x=x, shape = fit.gamma$estimate['shape'], rate=fit.gamma$estimate['rate']))
abline(v = qgamma(0.75, shape = fit.gamma$estimate['shape'], rate=fit.gamma$estimate['rate']), col='red')
length(boogg)
theta = rep(0,B)
B=1000
n=19
theta = rep(0,B)
theta
B=1000
n=19
theta = rep(0,B)
for(i in 1:B){
samle = rgamma(n, shape = fit.gamma$estimate['shape'], rate=fit.gamma$estimate['rate'])
th[i] = quantile(samle, 0.75)
}
B=1000
n=19
theta = rep(0,B)
for(i in 1:B){
samle = rgamma(n, shape = fit.gamma$estimate['shape'], rate=fit.gamma$estimate['rate'])
theta[i] = quantile(samle, 0.75)
}
alpha = 0.05
quantile(theta, alpha/2)
quantile(theta, 1-alpha/2)
c(quantile(theta, alpha/2), quantile(theta, 1-alpha/2))
s = sd(theta)
theta_n = quantile(boogg, 0.75)
s = sd(theta)
theta_n = quantile(boogg, 0.75)
lower = 2*theta_n - mean(theta) - s*qnorm(1-alpha/2)
upper = 2*theta_n - mean(theta) + s*qnorm(1-alpha/2)
c(lower, upper)
lower2 = 2*theta_n - quantile(theta, 1-alpha/2)
upper2 = 2*theta_n - quantile(theta, alpha/2)
c(lower2, upper2)
### e
require(boot)
boot.out.par = boot(boogg, sim = "parametric", ran.gen = simulate, mle = fit.gamma$estimate, R=B)
stat <- function(data){
return(quantile(data, 0.75))
}
boot.out.par = boot(boogg, sim = "parametric", ran.gen = simulate,
mle = fit.gamma$estimate, R=B, statistic = stat)
fit.gamma$estimate
fit.gamma$estimate[1]
### e
require(boot)
simulate <- function(data, mle){
n = length(data)
return(rgamma(n, shape = mle[1], mle[2]))
}
stat <- function(data){
return(quantile(data, 0.75))
}
boot.out.par = boot(boogg, sim = "parametric", ran.gen = simulate,
mle = fit.gamma$estimate, R=B, statistic = stat)
ci1.par <- boot.ci(boot.out.par, type = c("perc"))
ci2.par <- boot.ci(boot.out.par, type = c("norm"))
ci3.par <- boot.ci(boot.out.par, type = c("basic"))
ci1.par
ci2.par
ci3.par
### g
# According to the fitted Gamma model,
# what it the probability of an at least as long waiting time?
1-pgamma(57, shape = fit.gamma$estimate['shape'], rate=fit.gamma$estimate['rate'])
setwd("/Users/ludovicacattaneo/Desktop/Advanced Machine Learning/FakeNewsDetection")
